{"cells":[{"cell_type":"markdown","metadata":{"id":"O9K0o-g3wIfZ"},"source":["# Customizing what happens in `fit()`\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2020/04/15<br>\n","**Last modified:** 2020/04/15<br>\n","**Description:** Complete guide to overriding the training step of the Model class."]},{"cell_type":"markdown","metadata":{"id":"AXqukPcrwIfk"},"source":["## Introduction\n","\n","When you're doing supervised learning, you can use `fit()` and everything works\n","smoothly.\n","\n","When you need to write your own training loop from scratch, you can use the\n","`GradientTape` and take control of every little detail.\n","\n","But what if you need a custom training algorithm, but you still want to benefit from\n","the convenient features of `fit()`, such as callbacks, built-in distribution support,\n","or step fusing?\n","\n","A core principle of Keras is **progressive disclosure of complexity**. You should\n","always be able to get into lower-level workflows in a gradual way. You shouldn't fall\n","off a cliff if the high-level functionality doesn't exactly match your use case. You\n","should be able to gain more control over the small details while retaining a\n","commensurate amount of high-level convenience.\n","\n","When you need to customize what `fit()` does, you should **override the training step\n","function of the `Model` class**. This is the function that is called by `fit()` for\n","every batch of data. You will then be able to call `fit()` as usual -- and it will be\n","running your own learning algorithm.\n","\n","Note that this pattern does not prevent you from building models with the Functional\n","API. You can do this whether you're building `Sequential` models, Functional API\n","models, or subclassed models.\n","\n","Let's see how that works."]},{"cell_type":"markdown","metadata":{"id":"Kizv6RVfwIfm"},"source":["## Setup\n","Requires TensorFlow 2.2 or later."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"0JrdvmYFwIfo","executionInfo":{"status":"ok","timestamp":1663135944025,"user_tz":-270,"elapsed":3408,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"markdown","metadata":{"id":"MNAjJ6QzwIfr"},"source":["## A first simple example\n","\n","Let's start from a simple example:\n","\n","- We create a new class that subclasses `keras.Model`.\n","- We just override the method `train_step(self, data)`.\n","- We return a dictionary mapping metric names (including the loss) to their current\n","value.\n","\n","The input argument `data` is what gets passed to fit as training data:\n","\n","- If you pass Numpy arrays, by calling `fit(x, y, ...)`, then `data` will be the tuple\n","`(x, y)`\n","- If you pass a `tf.data.Dataset`, by calling `fit(dataset, ...)`, then `data` will be\n","what gets yielded by `dataset` at each batch.\n","\n","In the body of the `train_step` method, we implement a regular training update,\n","similar to what you are already familiar with. Importantly, **we compute the loss via\n","`self.compiled_loss`**, which wraps the loss(es) function(s) that were passed to\n","`compile()`.\n","\n","Similarly, we call `self.compiled_metrics.update_state(y, y_pred)` to update the state\n","of the metrics that were passed in `compile()`, and we query results from\n","`self.metrics` at the end to retrieve their current value."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_M2PbQWmwIfs","executionInfo":{"status":"ok","timestamp":1663136395425,"user_tz":-270,"elapsed":473,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}}},"outputs":[],"source":["\n","class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute the loss value\n","            # (the loss function is configured in `compile()`)\n","            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","        # Update metrics (includes the metric that tracks the loss)\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value\n","        return {m.name: m.result() for m in self.metrics}\n"]},{"cell_type":"markdown","metadata":{"id":"RFrxe0ZcwIft"},"source":["Let's try this out:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"H2KEXczGwIfv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663136427482,"user_tz":-270,"elapsed":1194,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"f8196f45-ba02-4a42-f83b-813ecb805eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 1s 2ms/step - loss: 0.7546 - mae: 0.7363\n","Epoch 2/3\n","32/32 [==============================] - 0s 2ms/step - loss: 0.3404 - mae: 0.4653\n","Epoch 3/3\n","32/32 [==============================] - 0s 1ms/step - loss: 0.2667 - mae: 0.4115\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5a29b94510>"]},"metadata":{},"execution_count":3}],"source":["import numpy as np\n","\n","# Construct and compile an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# Just use `fit` as usual\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.fit(x, y, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"3m6KgUWbwIfw"},"source":["## Going lower-level\n","\n","Naturally, you could just skip passing a loss function in `compile()`, and instead do\n","everything *manually* in `train_step`. Likewise for metrics.\n","\n","Here's a lower-level\n","example, that only uses `compile()` to configure the optimizer:\n","\n","- We start by creating `Metric` instances to track our loss and a MAE score.\n","- We implement a custom `train_step()` that updates the state of these metrics\n","(by calling `update_state()` on them), then query them (via `result()`) to return their current average value,\n","to be displayed by the progress bar and to be pass to any callback.\n","- Note that we would need to call `reset_states()` on our metrics between each epoch! Otherwise\n","calling `result()` would return an average since the start of training, whereas we usually work\n","with per-epoch averages. Thankfully, the framework can do that for us: just list any metric\n","you want to reset in the `metrics` property of the model. The model will call `reset_states()`\n","on any object listed here at the beginning of each `fit()` epoch or at the beginning of a call to\n","`evaluate()`."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Pih_CCGdwIfx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663136868365,"user_tz":-270,"elapsed":587,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"8af5948d-830c-4a89-81a3-f27b426dbb65"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.4181 - mae: 0.5269\n","Epoch 2/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.2362 - mae: 0.3919\n","Epoch 3/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.2247 - mae: 0.3824\n","Epoch 4/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.2193 - mae: 0.3779\n","Epoch 5/5\n","32/32 [==============================] - 0s 1ms/step - loss: 0.2142 - mae: 0.3737\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5a2da1cb10>"]},"metadata":{},"execution_count":4}],"source":["loss_tracker = keras.metrics.Mean(name=\"loss\")\n","mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n","\n","\n","class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute our own loss\n","            loss = keras.losses.mean_squared_error(y, y_pred)\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Compute our own metrics\n","        loss_tracker.update_state(loss)\n","        mae_metric.update_state(y, y_pred)\n","        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n","\n","    @property\n","    def metrics(self):\n","        # We list our `Metric` objects here so that `reset_states()` can be\n","        # called automatically at the start of each epoch\n","        # or at the start of `evaluate()`.\n","        # If you don't implement this property, you have to call\n","        # `reset_states()` yourself at the time of your choosing.\n","        return [loss_tracker, mae_metric]\n","\n","\n","# Construct an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","\n","# We don't passs a loss or metrics here.\n","model.compile(optimizer=\"adam\")\n","\n","# Just use `fit` as usual -- you can use callbacks, etc.\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.fit(x, y, epochs=5)\n"]},{"cell_type":"markdown","metadata":{"id":"qfxRnfxAwIfy"},"source":["## Supporting `sample_weight` & `class_weight`\n","\n","You may have noticed that our first basic example didn't make any mention of sample\n","weighting. If you want to support the `fit()` arguments `sample_weight` and\n","`class_weight`, you'd simply do the following:\n","\n","- Unpack `sample_weight` from the `data` argument\n","- Pass it to `compiled_loss` & `compiled_metrics` (of course, you could also just apply\n","it manually if you don't rely on `compile()` for losses & metrics)\n","- That's it. That's the list."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"_fTx7N10wIfy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663137119840,"user_tz":-270,"elapsed":1088,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"5fcc8936-7880-4335-9c32-22cdb3318c9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","32/32 [==============================] - 0s 1ms/step - loss: 0.1147 - mae: 0.3836\n","Epoch 2/3\n","32/32 [==============================] - 0s 1ms/step - loss: 0.1081 - mae: 0.3725\n","Epoch 3/3\n","32/32 [==============================] - 0s 1ms/step - loss: 0.1024 - mae: 0.3640\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5a29a59910>"]},"metadata":{},"execution_count":5}],"source":["\n","class CustomModel(keras.Model):\n","    def train_step(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        if len(data) == 3:\n","            x, y, sample_weight = data\n","        else:\n","            sample_weight = None\n","            x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)  # Forward pass\n","            # Compute the loss value.\n","            # The loss function is configured in `compile()`.\n","            loss = self.compiled_loss(\n","                y,\n","                y_pred,\n","                sample_weight=sample_weight,\n","                regularization_losses=self.losses,\n","            )\n","\n","        # Compute gradients\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics.\n","        # Metrics are configured in `compile()`.\n","        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n","\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# Construct and compile an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","# You can now use sample_weight argument\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","sw = np.random.random((1000, 1))\n","model.fit(x, y, sample_weight=sw, epochs=3)"]},{"cell_type":"markdown","metadata":{"id":"QWQgJfb6wIfz"},"source":["## Providing your own evaluation step\n","\n","What if you want to do the same for calls to `model.evaluate()`? Then you would\n","override `test_step` in exactly the same way. Here's what it looks like:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"5jRouNqwwIfz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663137185531,"user_tz":-270,"elapsed":665,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"aeb8fac8-ffbd-43c2-e597-6b39efd370a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["32/32 [==============================] - 0s 1ms/step - loss: 0.8602 - mae: 0.8104\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.8602128028869629, 0.8103616833686829]"]},"metadata":{},"execution_count":6}],"source":["\n","class CustomModel(keras.Model):\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","        # Compute predictions\n","        y_pred = self(x, training=False)\n","        # Updates the metrics tracking the loss\n","        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_pred)\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {m.name: m.result() for m in self.metrics}\n","\n","\n","# Construct an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)\n","model.compile(loss=\"mse\", metrics=[\"mae\"])\n","\n","# Evaluate with our custom test_step\n","x = np.random.random((1000, 32))\n","y = np.random.random((1000, 1))\n","model.evaluate(x, y)"]},{"cell_type":"markdown","metadata":{"id":"Vf2lrvRjwIf0"},"source":["## Wrapping up: an end-to-end GAN example\n","\n","Let's walk through an end-to-end example that leverages everything you just learned.\n","\n","Let's consider:\n","\n","- A generator network meant to generate 28x28x1 images.\n","- A discriminator network meant to classify 28x28x1 images into two classes (\"fake\" and\n","\"real\").\n","- One optimizer for each.\n","- A loss function to train the discriminator.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TdB4hfapwIf0","executionInfo":{"status":"ok","timestamp":1663138022467,"user_tz":-270,"elapsed":447,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}}},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","# Create the discriminator\n","discriminator = keras.Sequential(\n","    [\n","        keras.Input(shape=(28, 28, 1)),\n","        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.GlobalMaxPooling2D(),\n","        layers.Dense(1),\n","    ],\n","    name=\"discriminator\",\n",")\n","\n","# Create the generator\n","latent_dim = 128\n","generator = keras.Sequential(\n","    [\n","        keras.Input(shape=(latent_dim,)),\n","        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n","        layers.Dense(7 * 7 * 128),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Reshape((7, 7, 128)),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n","        layers.LeakyReLU(alpha=0.2),\n","        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n","    ],\n","    name=\"generator\",\n",")"]},{"cell_type":"code","source":["generator.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58x__GZ3wE1d","executionInfo":{"status":"ok","timestamp":1663138071884,"user_tz":-270,"elapsed":15,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"404c6295-f902-4dd9-c0f0-c23e49de5ed5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_5 (Dense)             (None, 6272)              809088    \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 6272)              0         \n","                                                                 \n"," reshape (Reshape)           (None, 7, 7, 128)         0         \n","                                                                 \n"," conv2d_transpose (Conv2DTra  (None, 14, 14, 128)      262272    \n"," nspose)                                                         \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 128)       0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2DT  (None, 28, 28, 128)      262272    \n"," ranspose)                                                       \n","                                                                 \n"," leaky_re_lu_4 (LeakyReLU)   (None, 28, 28, 128)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 28, 28, 1)         6273      \n","                                                                 \n","=================================================================\n","Total params: 1,339,905\n","Trainable params: 1,339,905\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"qbeVle3XwIf1"},"source":["Here's a feature-complete GAN class, overriding `compile()` to use its own signature,\n","and implementing the entire GAN algorithm in 17 lines in `train_step`:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"dARawLRjwIf2","executionInfo":{"status":"ok","timestamp":1663138570487,"user_tz":-270,"elapsed":793,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}}},"outputs":[],"source":["\n","class GAN(keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            real_images = real_images[0]\n","        # Sample random points in the latent space\n","        batch_size = tf.shape(real_images)[0]\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Decode them to fake images\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # Assemble labels discriminating real from fake images\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        # Sample random points in the latent space\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Assemble labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator (note that we should *not* update the weights\n","        # of the discriminator)!\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"]},{"cell_type":"markdown","metadata":{"id":"_-ic4hO5wIf2"},"source":["Let's test-drive it:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"frxTOMfkwIf3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663138896101,"user_tz":-270,"elapsed":264850,"user":{"displayName":"Soheil Koohi","userId":"04529429644948530935"}},"outputId":"95b1e744-5aeb-4a8f-8774-15c0028cbe2c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","100/100 [==============================] - 251s 2s/step - d_loss: 0.5119 - g_loss: 0.8192\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5a25320ed0>"]},"metadata":{},"execution_count":11}],"source":["# Prepare the dataset. We use both the training & test MNIST digits.\n","batch_size = 64\n","(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n","all_digits = np.concatenate([x_train, x_test])\n","all_digits = all_digits.astype(\"float32\") / 255.0\n","all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n","dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","gan.compile(\n","    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n","    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",")\n","\n","# To limit the execution time, we only train on 100 batches. You can train on\n","# the entire dataset. You will need about 20 epochs to get nice results.\n","gan.fit(dataset.take(100), epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"QBfcMNbNwIf3"},"source":["The ideas behind deep learning are simple, so why should their implementation be painful?"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/guides/ipynb/customizing_what_happens_in_fit.ipynb","timestamp":1661897036159}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}